=====================================================================================================================================================
#	Definitions
=====================================================================================================================================================
#.	Big Data : Big data is a broad term for data sets so large or complex that traditional data processing applications are inadequate. 
#.	Hadoop : Solution for deciphering the avalanche of Big Data

=====================================================================================================================================================
#	Hadoop admin
=====================================================================================================================================================
#.	Single node setup : https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html
#.	Set following in bash profile,
	-------------------------------------------------------------------------------------------------------------------------------------------------
	# Hadoop variables
	export HADOOP_HOME=/root/hadoop
	export HADOOP_INSTALL=/root/hadoop
	export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
	export HADOOP_COMMON_HOME=$HADOOP_INSTALL
	export HADOOP_HDFS_HOME=$HADOOP_INSTALL
	export YARN_HOME=$HADOOP_INSTALL
	#Also used by Apache Spark
	export HADOOP_CONF_DIR=$HADOOP_INSTALL/etc/hadoop
	
	# Packer.io/Maven/Ant/Splunk executables in PATH
	export PATH=$HOME/bin:/root/downloads/packer:/root/maven/bin:/root/amit/apache-ant-1.9.4/bin:/opt/chef/embedded/bin:$SPLUNK_HOME/bin:/usr/java/jdk1.7.0_17/bin/:$HADOOP_INSTALL/bin:$HADOOP_INSTALL/sbin:$PATH
	-------------------------------------------------------------------------------------------------------------------------------------------------
#.	Start NameNode and Secondry NameNode using command:  sbin/start-dfs.sh and access namenode at http://192.168.148.76:50070/
#.	Start Yarn using command : sbin/start-yarn.sh Resource Manager/Yarn : http://192.168.148.76:8088/cluster
#.	JAVA issue 
	#.	cd /usr/lib/jvm
	#.	sudo ln -s /usr/java/jdk1.7.0_17/ java
#.	Hadoop connection refused issue : http://stackoverflow.com/questions/29192088/java-net-connectexception-connection-refused-in-hadoop-while-using-shell-comman

=====================================================================================================================================================
#	Spark 
=====================================================================================================================================================
#.	Running on yarn : http://spark.apache.org/docs/latest/running-on-yarn.html
#.	To submit a spark job : ./bin/spark-submit --class com.inbravo.spark.JavaWordCount --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 1 --queue default ../amit/spark/ds.jar /user/input.txt
=====================================================================================================================================================
#	HDFS Commands
=====================================================================================================================================================
#.	Create new directory on hdfs : hdfs dfs -mkdir /user
#.	List the contents of a directory : hdfs dfs -ls /user
#.	File copy :  hadoop fs -copyFromLocal /root/amit/hadoop-book/input/docs/quangle.txt hdfs://localhost:9000/user/amit/quangle.txt
#.	File put : hadoop fs -put input/ncdc/sample.txt /user/test
#.	File print : hadoop fs -cat /user/root/output/part-r-00000
=====================================================================================================================================================
#	YARN
=====================================================================================================================================================
#.	To check status of an application : yarn logs -applicationId application_1453903755565_0008 

=====================================================================================================================================================
#	My development enviornment details
=====================================================================================================================================================
#.	Run job on local 
	#.	export HADOOP_CLASSPATH=ds.jar 
	#.	hadoop MaxTemperature ../../input/ncdc/sample.txt output
	#.	hadoop com.inbravo.hadoop.MaxTemperature ../hadoop-book/input/ncdc/sample.txt output
	#.	Output is available at /output/part-r-00000
#.	Run job on local cluster
	#.	Put sample file on hdfs : hadoop fs -put input/ncdc/sample.txt /user/test
	#.	Now run job with cluster : hadoop com.inbravo.hadoop.MaxTemperatureDriver -conf conf/hadoop-cluster.xml /user/test/sample.txt output
	#.	Output is available at : hadoop fs -cat /user/root/output/part-r-00000

=====================================================================================================================================================
#	Processing tools information
=====================================================================================================================================================
#.	Spark : It performs Data-Parallel computations. 
	#.	Data parallelism is achieved when each processor performs the same task on different pieces of distributed data
	#.	Spark white paper : https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf
#.	Storm : It performs Task-Parallel computations. 
	#.	Task parallelism is achieved when each processor executes a different thread (or process) on the same or different data.
	#.	Documentation : http://spark.apache.org/docs/latest/
#.	Tachyon
	#.	Homepage: http://tachyon-project.org/documentation/index.html 
#.	Spring XD
	#.	http://projects.spring.io/spring-xd/
#.	Nifi
	#.	https://nifi.apache.org/docs.html
=====================================================================================================================================================
#	Comparision
=====================================================================================================================================================
#.	https://amplab.cs.berkeley.edu/benchmark/
=====================================================================================================================================================
#	Hadoop Certification
=====================================================================================================================================================
#.	Horten : http://hortonworks.com/training/class/hdp-certified-java-developer-exam/
=====================================================================================================================================================
#	Spark Certification
=====================================================================================================================================================
#.	Learning-Spark is must(Learning Spark Lightning-Fast Big Data Analysis) One should go with this book thoroughly. 
	This would make you comfortable with spark architecture and framework and would be enough for all the theoretically questions.
#.	Try all the example given in this book(Learning-Spark). Including all RDD APIs, Spark Streaming and Spark SQL.
#.	Go through with all the examples given @ http://spark.apache.org/docs/latest/quick-start.html . 
	I did Mllib and Graph-X from here. Here they have very well explanation of all the topics.
#.	Make sure you do enough hands on before appearing for the exam. 
	Most of the questions were related to programming so must have through understanding how API works in detail.
#.	Last but not lest I see many people are asking about which language they should focus (Scala/Python or Java). 
	Certification is more on programing oriented and they are covering all scala, python, java & sql [Spark core(more question), Dataframe, streaming & machine learning (2 question), Graphx(2 Question)].
	But to answer the question I don't think you have to be expert in programming language and basic understanding would work if you're clear about how Spark API works.
#.	HERE'RE TOPICS ONE MUST COVER:
    #.	Different transformation & actions in RDD
    #.	Pair RDD and Dstreams
    #.	Batch and window sizing in spark streaming
    #.	Various Joins and Cartesian operations in RDD
    #.	Broadcast and accumulator
    #.	Word count example (Specially in Java)
    #.	pyspark especially set and join
    #.	schemaRDD in spark sql
    #.	Lineage and memory usage
    #.	MLLib : Regression, K-means and Clustering
    #.	Graph-X: Spark Quick Start guide would be enough.

